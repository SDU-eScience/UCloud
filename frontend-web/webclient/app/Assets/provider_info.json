{
    "providers": [
        {
            "id": "aau",
            "title": "DeiC Interactive HPC (AAU)",
            "shortTitle": "AAU/VM",
            "logo": "aau_openstack.png",
            "shortDescription": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "description": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "url": "https://docs.cloud.sdu.dk/",
            "texts": [
                {
                    "description": "# Interactive HPC\nThe interactive HPC cluster consists of 90 nodes, with a total of 2896 cores and 37 TB of memory.\nThere are also a few Nvidia V100 and A100 graphical processing units for accelerating certain workloads.\n The theoretical peak performance is around 118 teraflops.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Nodes\n- Dell PowerEdge C6420, 384/768 GB DDR 4-2666, 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz\n- 64-core virtual machine 256 GBs of RAM\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA T4 GPUs\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA A10 GPUs",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "aau-k8",
            "title": "DeiC Interactive HPC (AAU/K8)",
            "logo": "aau_k8.png",
            "shortTitle": "AAU/K8",
            "shortDescription": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "description": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "url": "https://docs.cloud.sdu.dk/",
            "texts": [
                {
                    "description": "# Interactive HPC\nThe interactive HPC cluster consists of 90 nodes, with a total of 2896 cores and 37 TB of memory.\nThere are also a few Nvidia V100 and A100 graphical processing units for accelerating certain workloads.\n The theoretical peak performance is around 118 teraflops.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Nodes\n- Dell PowerEdge C6420, 384/768 GB DDR 4-2666, 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz\n- 64-core virtual machine 256 GBs of RAM\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA T4 GPUs\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA A10 GPUs",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "k8",
            "title": "K8s Dev Provider",
            "logo": "sdu.png",
            "shortTitle": "SDU/K8",
            "shortDescription": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "description": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "url": "https://docs.cloud.sdu.dk/",
            "texts": [
                {
                    "description": "# Interactive HPC\nThe interactive HPC cluster consists of 90 nodes, with a total of 2896 cores and 37 TB of memory.\nThere are also a few Nvidia V100 and A100 graphical processing units for accelerating certain workloads.\n The theoretical peak performance is around 118 teraflops.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Nodes\n- Dell PowerEdge C6420, 384/768 GB DDR 4-2666, 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz\n- 64-core virtual machine 256 GBs of RAM\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA T4 GPUs\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA A10 GPUs",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "K8",
            "title": "K8s Dev Provider",
            "logo": "sdu.png",
            "shortTitle": "SDU/K8",
            "shortDescription": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "description": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "url": "https://docs.cloud.sdu.dk/",
            "texts": [
                {
                    "description": "# Interactive HPC\nThe interactive HPC cluster consists of 90 nodes, with a total of 2896 cores and 37 TB of memory.\nThere are also a few Nvidia V100 and A100 graphical processing units for accelerating certain workloads.\n The theoretical peak performance is around 118 teraflops.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Nodes\n- Dell PowerEdge C6420, 384/768 GB DDR 4-2666, 2x Intel Xeon Gold 6130 16-Core @ 2.10GHz\n- 64-core virtual machine 256 GBs of RAM\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA T4 GPUs\n- 40-core virtual machine with 160 GBs of RAM and 4 NVIDIA A10 GPUs",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "ucloud",
            "title": "DeiC Interactive HPC (SDU)",
            "logo": "sdu.png",
            "shortTitle": "SDU/K8",
            "shortDescription": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "description": "Interactive digital research environment built to support the needs of researchers for both computing and data management, throughout all the data life cycle.",
            "url": "https://docs.cloud.sdu.dk/",
            "texts": [
                {
                    "description": "# Interactive HPC\nThe interactive HPC cluster consists of 90 nodes, with a total of 2896 cores and 37 TB of memory.\nThere are also a few Nvidia V100 and A100 graphical processing units for accelerating certain workloads.\n The theoretical peak performance is around 118 teraflops.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Compute nodes\n\n**81 Standard Nodes (`u1-standard`):**\n\n- Dell PowerEdge C6420\n- 2x Intel Xeon Gold 6130 @ 2.10 Ghz (total of 64 vCPU)\n- 384 GB DDR4-2400\n\n**6 Fat Nodes (`u1-fat`):**\n\n- Dell PowerEdge C6420\n- 2x Intel Xeon Gold 6130 @ 2.10 Ghz (total of 64 vCPU)\n- 768 GB DDR4-2400\n\n**2 GPU Nodes (`u1-gpu`):**\n\n- Dell PowerEdge C4140\n- 2x Intel Xeon Gold 6230 @ 2.10 Ghz (total of 80 vCPU)\n- 192 GB DDR4-2933\n- 4x NVIDIA Tesla V100-SXM2, 32 GB\n\n**1 GPU Node (`u2-gpu`):**\n\n- HPE ProLiant XL675d Gen10+\n- 2x AMD EPYC 7F72 @ 3.20 Ghz (total of 96 vCPU)\n- 2048 GB DDR4-2933\n- 8x NVIDIA Ampere A100-PCIe, 40 GB\n\n**4 GPU Nodes (`u3-gpu`):**\n\n- Lenovo ThinkSystem SR675 V3\n- 2x AMD EPYC 9454 @ 2.75 Ghz (total of 192 vCPU)\n- 768 GB DDR5-4800\n- 4x NVIDIA Hopper H100-SXM5, 80 GB\n",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "hippo",
            "title": "DeiC Large Memory HPC (SDU)",
            "logo": "hippo.png",
            "shortTitle": "Hippo",
            "shortDescription": "The DeiC Large Memory HPC system is a system consisting of large memory nodes (up to 4 TB RAM per node).",
            "description": "The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node) configured as a traditional Slurm cluster.",
            "url": "https://docs.hpc-type3.sdu.dk/",
            "texts": [
                {
                    "description": "# Large Memory HPC\nThe large memory HPC system is a specialised system where each node is equipped with a large amount of memory (up to 4 TB).\nThe system is used for workloads that cannot easily be parallelised and distributed across multiple nodes.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Compute nodes\n\n__Updated at: 08/05/2024. For an up-to-date version see [https://docs.hpc-type3.sdu.dk/intro/hardware.html](https://docs.hpc-type3.sdu.dk/intro/hardware.html).__\n\n**4x Lenovo ThinkSystem SR645 (called `hm1` in Slurm)**\n\n- 4096 GB DDR4-2400 LRDIMM\n- 2x AMD EPYC 7742 64-Core @ 2.25Ghz\n- 480 GB SSD\n- 7.68 TB NVMe (only two nodes)\n\n**10x Dell PowerEdge R6525 (called `hm2` in Slurm)**\n\n- 1024 GB DDR4-2933\n- 2x AMD EPYC 7713 64-Core @ 2.0Ghz\n- 480 GB SSD\n\n**20x Lenovo ThinkSystem SR645 V3 (called `hm3` in Slurm)**\n\n- 1536 GB DDR5-4800\n- 2x AMD EPYC 9534 64-Core @ 2.45Ghz\n- 480 GB SSD",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "slurm",
            "title": "Slurm Dev Provider",
            "logo": "hippo.png",
            "shortTitle": "Hippo",
            "shortDescription": "The DeiC Large Memory HPC system is a system consisting of large memory nodes (up to 4 TB RAM per node).",
            "description": "The DeiC Large Memory HPC system is a system consisting of large memory nodes (between 1 and 4 TB RAM per node) configured as a traditional Slurm cluster.",
            "url": "https://docs.hpc-type3.sdu.dk/",
            "texts": [
                {
                    "description": "# Large Memory HPC\nThe large memory HPC system is a small and specialised four-node system where each node is equipped with 4 TB of memory and 128 cores.\nThe system is used for workloads that cannot easily be parallelised and distributed across multiple nodes.",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "### Nodes\n- Lenovo ThinkSystem SR645\n- 4096 GB DDR4-2400 LRDIMM\n- 2x AMD EPYC 7742 64-Core @ 2.25 GHz",
                    "image": "/Images/ucloud-9.svg"
                }
            ]
        },
        {
            "id": "sophia",
            "title": "DeiC Throughput HPC (DTU)",
            "logo": "sophia.png",
            "shortTitle": "Sophia",
            "shortDescription": "The Sophia cluster is a collaboration between DTU, DTU Wind Energy and DTU Mechanics.",
            "description": "The Sophia cluster is a collaboration between DTU, DTU Wind Energy and DTU Mechanics.",
            "url": "https://dtu-sophia.github.io/docs/",
            "texts": [
                {
                    "description": "# Compute nodes\nThe Sophia HPC cluster consists of 516 computational nodes of which 484 are 128 GB RAM nodes and 32 are 256 GB RAM nodes. Each node is a powerful x86-64 computer, equipped with 32 physical cores (2 x sixteen-core AMD EPYC 7351).",
                    "image": "/Images/ucloud-1.png"
                },
                {
                    "description": "# Specs\n- Primary purpose: High Performance Computing\n- Architecture of compute nodes: x86-64\n- Operating system: CentOS 7 Linux\n- Compute nodes in total: 516\n- Processor: 2 x AMD EPYC 7351, 2.9 GHz, 16 cores\n- RAM (484 nodes): 128 GB, 4 GB per core, DDR4@2666 MHz\n- RAM (32 nodes): 256 GB, 8 GB per core, DDR4@2666 MHz\n- Local disk drive: no\n- Compute network / Topology: InfiniBand EDR / Fat tree\n # In total\n- Total theoretical peak performance (Rpeak): ~384 TFLOPS (516 nodes x 32 cores x 2.9GHz x 8 FLOP/cycle)\n- Total amount of RAM: 69 TB",
                    "image": "/Images/ucloud-9.svg"
                },
                {
                    "description": "# High-speed interconnect\nThe nodes are interlinked by InfiniBand and 10 Gbps Ethernet networks.\nSophia's high-speed, low-latency interconnect is Mellanox EDR (100Gbps) Infiniband. Frontend-, compute-, and burst buffer nodes each have Mellanox' ConnectX-5 adapter card installed.",
                    "image": "/Images/ucloud-1.png"
                }
            ]
        },
        {
            "id": "lumi-sdu",
            "title": "LUMI (SDU)",
            "logo": "lumi.png",
            "shortTitle": "LUMI (SDU)",
            "shortDescription": "LUMI is one of the three European pre-exascale supercomputers.",
            "description": "LUMI is one of the three European pre-exascale supercomputers. It's an HPE Cray EX supercomputer consisting of several hardware partitions targeted different use cases.",
            "url": "https://www.lumi-supercomputer.eu/",
            "texts": [
                {
                    "description": "### Overview\n\nLUMI is the fastest supercomputer in Europe and third fastest globally (the [Top500 list published in June 2022](https://top500.org/lists/top500/2022/06/)). LUMI is also the third greenest supercomputer on the planet (the Green500 list published in June 2022).\n\nLUMI has a sustained computing power of 375 petaflops (HPL, High-Performance Linpack) which equals a theoretical computing power of more than 550 petaflops which means 550 quintillion calculations per second. LUMI is also one of the worldâ€™s leading platforms for artificial intelligence.",
                    "image": "/Images/lumi-desc1.png"
                },
                {
                    "description": "### Compute Nodes\n\nLUMI-C: 1536 CPU based compute nodes, each with 2x AMD EPYC 7763 (128 cores), 256/512/1024GiB ram, 200Gp/s slingshot network\n\nLUMI-G: 2560 GPU based nodes, each one AMD EPYC 7A53 CPU (64 cores) and 4x GPUs AMD Instinct MI250X 128GB HBM2e, 4x 200Gp/s slingshot network",
                    "image": "/Images/milan-overview.svg"
                },
                {
                    "description": "### Storage\n\nLUMI-P: 4x Lustre filesystems based on HDD, each with a capacity of 20 PB. Aggregate bandwidth of 240 GB/s.\n\nLUMI-F: fast flash-based Lustre filesystem with a storage capacity of 7 PB and an aggregate bandwidth of 1740 GB/s.",
                    "image": "/Images/lustre-overview.svg"
                }
            ]
        },
        {
            "id": "go-slurm",
            "title": "Slurm (Go IM2)",
            "logo": "gopher.png",
            "shortTitle": "Slurm/IM2",
            "shortDescription": "Development version of IM2.",
            "description": "Development version of IM2.",
            "url": "https://github.com/sdu-escience/ucloud",
            "texts": [
                {
                    "description": "Nothing to show here. This is just a development version if IM2.",
                    "image": "/Images/ucloud-1.png"
                }
            ]
        }
    ]
}
